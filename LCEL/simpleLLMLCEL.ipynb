{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Simple LLM Application with LCEL\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "After seeing this video, you'll have a high level overview of:\n",
    "\n",
    "- Using language models\n",
    "\n",
    "- Using PromptTemplates and OutputParsers\n",
    "\n",
    "- Using LangChain Expression Language (LCEL) to chain components together\n",
    "\n",
    "- Debugging and tracing your application using LangSmith\n",
    "\n",
    "- Deploying your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "\n",
    "# grop_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# grop_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Downlaod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x114395ff0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x114396c80>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model='gemma2-9b-it', api_key=grop_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLMs (Large Language Models) are revolutionizing the field of data science, opening up exciting new job profiles. Here are some examples:\\n\\n**Directly Leveraging LLMs:**\\n\\n* **AI Data Scientist:** This role focuses on building and deploying AI models, including those powered by LLMs. Tasks include data preprocessing, feature engineering, model training, evaluation, and deployment.\\n* **Prompt Engineer:**  A specialized role focused on crafting effective prompts for LLMs to elicit desired outputs. They understand the nuances of language and how to guide the model towards specific tasks and results.\\n* **LLM Trainer/Fine-Tuner:**  These professionals specialize in training and fine-tuning LLMs on specific datasets for specialized applications. They have expertise in machine learning algorithms, data augmentation techniques, and model optimization.\\n\\n**Indirectly Influenced by LLMs:**\\n\\n* **Data Analyst:** LLMs can automate data cleaning, summarization, and visualization tasks, freeing up data analysts to focus on deeper insights and storytelling.\\n* **Data Engineer:** LLMs can assist in automating data pipeline tasks, generating code for data transformations, and improving data quality.\\n* **Machine Learning Engineer:** LLMs can accelerate the development of machine learning models by automating code generation, feature extraction, and model selection.\\n\\n**Emerging Roles:**\\n\\n* **AI Ethicist:** As LLMs become more powerful, ethical considerations become paramount. This role focuses on ensuring responsible development and deployment of AI, addressing bias, fairness, and transparency issues.\\n* **AI Conversational Designer:** LLMs are powering chatbots and virtual assistants. This role focuses on designing natural and engaging conversational experiences.\\n* **AI Product Manager:** This role bridges the gap between technical development and business needs, defining the product roadmap for LLM-powered applications.\\n\\n**Essential Skills:**\\n\\nRegardless of the specific role, data scientists working with LLMs will need a strong foundation in:\\n\\n* **Machine Learning:** Understanding the fundamentals of machine learning algorithms and model training.\\n* **Natural Language Processing (NLP):**  Knowledge of NLP techniques and concepts is crucial for working with text data.\\n* **Programming:** Proficiency in Python or other relevant programming languages is essential.\\n* **Data Analysis and Visualization:**  Ability to analyze and interpret data, and communicate insights effectively.\\n* **Problem-Solving and Critical Thinking:**  LLMs are powerful tools, but they require human guidance and critical thinking to solve complex problems.\\n\\n\\nThe field of data science is rapidly evolving, and LLMs are at the forefront of this transformation. By acquiring the necessary skills and knowledge, data professionals can position themselves to thrive in these exciting new roles. \\n', response_metadata={'token_usage': {'completion_tokens': 548, 'prompt_tokens': 24, 'total_tokens': 572, 'completion_time': 1.154003006, 'prompt_time': 0.002329414, 'queue_time': None, 'total_time': 1.15633242}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1bbcd4db-fd0c-4148-87af-bd5eb314725b-0', usage_metadata={'input_tokens': 24, 'output_tokens': 548, 'total_tokens': 572})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what are job profiles for LLm in data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are expert real state agent and proffessional in this'),\n",
       " HumanMessage(content='how to calculate builtup are when efficiency and carpet is give provide formula')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage , SystemMessage\n",
    "\n",
    "\n",
    "prompt_template = [\n",
    "        SystemMessage(content=\"you are expert real state agent and proffessional in this\"),\n",
    "        HumanMessage(content=\"how to calculate builtup are when efficiency and carpet is give provide formula\")\n",
    "]\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(prompt_template)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m StrOutputParser()\n\u001b[0;32m----> 5\u001b[0m output\u001b[38;5;241m.\u001b[39minvoke(\u001b[43mresponse\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output = StrOutputParser()\n",
    "\n",
    "output.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m|\u001b[39moutput\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "chain = model|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're asking about calculating built-up area when you're given the efficiency and carpet area. Here's the breakdown:\\n\\n**Understanding the Terms**\\n\\n* **Carpet Area:** This is the usable floor area within the walls of a property, excluding balconies, external walls, and internal walls. \\n* **Built-up Area:** This is the total covered area of a property, including the carpet area, balconies, external walls, and internal walls.\\n* **Efficiency:** This is the ratio of the built-up area to the carpet area. It's expressed as a percentage.\\n\\n**Formula**\\n\\n**Built-up Area = Carpet Area * (100 / Efficiency)**\\n\\n**Example**\\n\\nLet's say you have a property with:\\n\\n* Carpet Area: 1000 square feet\\n* Efficiency: 80%\\n\\n**Calculation:**\\n\\nBuilt-up Area = 1000 square feet * (100 / 80) = 1250 square feet\\n\\n**Important Considerations**\\n\\n* **Local Standards:**  Building codes and regulations can vary by location.  Make sure you're using the correct definition of built-up area and efficiency as defined in your area.\\n* **Accuracy:**  Efficiency percentages can be influenced by factors like the design of the property (lots of balconies, internal walls) and even how the area is measured. Always verify the provided efficiency percentage with the property developer or local authorities.\\n\\n\\nLet me know if you have any more questions or would like to work through another example!\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_prompt = \"translate the following text into {language}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages (\n",
    "    [(\"system\" , generic_prompt),(\"user\",\"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='translate the following text into hindi'), HumanMessage(content='Hello my name is Sachin')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt.invoke({\"language\":\"hindi\",\"text\":\"Hello my name is Sachin\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='translate the following text into hindi'),\n",
       " HumanMessage(content='Hello my name is Sachin')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|model|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour, je m'appelle Sachin. \\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\":\"French\",\"text\":\"Hello my name is Sachin\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], template='translate the following text into {language} based on {text}'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generic_prompt1 = \"translate the following text into {language} based on {text}\"\n",
    "prompt2 =ChatPromptTemplate.from_template(generic_prompt1)\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='translate the following text into French based on Hello my name is Sachin')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt2.invoke({\"language\":\"French\",\"text\":\"Hello my name is Sachin\"})\n",
    "result.to_messages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = prompt2|model|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Review > Translation > Select Translation. Select your language to view the translation.  Choose.**\\n\\n**The translated content will replace the highlighted content from Step 1.** \\n\\n\\nLet me know if you'd like me to translate anything else! \\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"language\":\"english\",\"text\":\"पुनरावलोकन > भाषांतर > भाषांतर निवड निवडा. भाषांतर पाहण्यासाठी तुमची भाषा निवडा. घाला निवडा . भाषांतरित मजकूर तुम्ही चरण 1 मध्ये हायलाइट केलेल्या मजकुराची जागा घेईल.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chennai = \"\"\"01-04-2012 அå² உயßÚதÜபØட லÚå வகாØ} மÜ®,\n",
    "09-06-2017 அå² ¯தà ஒேர ராக 33 சத தÝ ¤ைறÔகÜபØட«.\n",
    "ேம³Ý, äபைன, நåெகாைட, பமாäறÝ மä²Ý ¤©Ýப\n",
    "உ²Üனßக´Ô¤ இைடேய அàலாத ஏäபா© ஆவணÕக´Ôகான\n",
    "ப¶Ô கØடணÝ ஒ± சத தÚ±Û« நாå¤ சத தமாக\n",
    "உயßÚதÜபØட«. சÛைத மÜä¤ ஏäப வகாØ} மÜைப\n",
    "உயßÚத¶Ý, ப¶Ô கØடணÚைதÔ ¤ைறÔக¶Ý பàேவ²\n",
    "தரÜனட±Û« ெதாடßÛ« ேகாÔைககã வÛத வÙணÝ\n",
    "உãளன. இÛத ேகாÔைககைள ஏä², வகாØ} மÜà\n",
    "±ÚதÚைதÜ பÛ«ைரÔக ஒ± ¤µைவ அர¦ அைமÚ«ãள«.\n",
    "இÔ¤µ லஅளைவ எÙ வாயாக ±ÚதÝ ேமäெகாãள கால\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cehnnai = chain2.invoke({\"language\":\"english\",\"text\":chennai})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 01-04-2012, a directive was issued stating that applications for University admissions should be made through online mode only.\n",
      "\n",
      "On 09-06-2017, an amendment was made stating that 33% of the seats should be reserved for the government quota.\n",
      "\n",
      "Hence, the counseling for admission to universities will be conducted considering the government quota of 33% seats.\n",
      "\n",
      "Applications received from students belonging to the backward classes, minority communities, and other categories will be considered for admission based on the available seats. \n",
      "\n",
      "The date for the counseling process will be announced later. We request all students who have applied for admission to note this information and wait for further updates.\n",
      "\n",
      "\n",
      "\n",
      "Please let me know if you need any further clarification or translation! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cehnnai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
